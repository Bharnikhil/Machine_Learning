{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fcadaac",
   "metadata": {},
   "source": [
    "## ðŸ“Œ Ridge vs Lasso Regression\n",
    "\n",
    "Ridge and Lasso Regression are two popular techniques in machine learning used for **regularizing linear models** to avoid overfitting and improve predictive performance. Both methods add a **penalty term** to the modelâ€™s cost function to constrain the coefficients, but they differ in how they apply this penalty.\n",
    "\n",
    "- **Ridge Regression (L2 regularization):**  \n",
    "  Adds the **squared magnitude** of the coefficients as a penalty.\n",
    "\n",
    "- **Lasso Regression (L1 regularization):**  \n",
    "  Introduces a penalty based on the **absolute value** of the coefficients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654f1f03",
   "metadata": {},
   "source": [
    "## ðŸ“Œ What is Ridge Regression (L2 Regularization) Method?\n",
    "\n",
    "Ridge regression, also known as **L2 regularization**, is a technique used in linear regression to **prevent overfitting** by adding a penalty term to the loss function. This penalty is proportional to the **square of the magnitude of the coefficients (weights)**.\n",
    "\n",
    "Ridge Regression is a version of linear regression that includes a penalty to prevent the model from overfitting, especially when there are **many predictors** or **not enough data**.\n",
    "\n",
    "The standard loss function (mean squared error) is modified to include a regularization term:\n",
    "\n",
    "Loss = MSE + Î» Î£ (wáµ¢Â²) for i = 1 to n\n",
    "\n",
    "Here:  \n",
    "- Î» = regularization parameter that controls the strength of the penalty.  \n",
    "- wáµ¢ = model coefficients (weights).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e740113",
   "metadata": {},
   "source": [
    "ðŸ”¹ **How coefficients shrink**  \n",
    "\n",
    "The model tries to minimize both the **MSE** and the **penalty**:  \n",
    "`Loss = MSE + Î» Î£ wáµ¢Â²`  \n",
    "\n",
    "If a coefficient \\(w_i\\) is very large, the penalty \\(\\lambda w_i^2\\) becomes huge.  \n",
    "To keep the loss small, the model reduces the size of the coefficients (slopes).  \n",
    "This is why Ridge makes coefficients smaller (shrinks them) compared to plain linear regression.  \n",
    "\n",
    "---\n",
    "\n",
    "ðŸ”¹ **Why they donâ€™t become exactly zero**  \n",
    "\n",
    "The penalty is based on squares (\\(w_i^2\\)).  \n",
    "Squaring makes the cost curve **smooth and round**.  \n",
    "When optimization happens (using gradient descent or a matrix solution), coefficients are pushed toward zero but never forced to touch it.  \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75673895",
   "metadata": {},
   "source": [
    "## ðŸ“Œ Lasso Regression (L1 Regularization)\n",
    "\n",
    "Lasso regression, also known as **L1 regularization**, is a linear regression technique that adds a penalty to the loss function to prevent overfitting. This penalty is based on the **absolute values of the coefficients**.  \n",
    "\n",
    "Lasso regression modifies linear regression by including a penalty equal to the absolute value of the coefficient magnitudes.  \n",
    "By encouraging **sparsity**, this L1 regularization term reduces overfitting and helps some coefficients become exactly **zero**, hence facilitating **feature selection**.  \n",
    "\n",
    "---\n",
    "\n",
    "ðŸ”¹ **Loss Function:**  \n",
    "\n",
    "Loss = MSE + $\\lambda \\sum_{i=1}^n |w_i|$\n",
    "\n",
    "Where:  \n",
    "- $\\lambda$ = regularization parameter controlling penalty strength.  \n",
    "- $w_i$ = model coefficients (slopes).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41f2d2d",
   "metadata": {},
   "source": [
    "| Characteristic      | Ridge Regression                                                                 | Lasso Regression                                                                 |\n",
    "|---------------------|----------------------------------------------------------------------------------|----------------------------------------------------------------------------------|\n",
    "| **Regularization Type** | Applies L2 regularization, adding a penalty term proportional to the **square of the coefficients** | Applies L1 regularization, adding a penalty term proportional to the **absolute value of the coefficients** |\n",
    "| **Feature Selection**   | Does **not** perform feature selection. All predictors are retained, although their coefficients are reduced in size to minimize overfitting | Performs **automatic feature selection**. Less important predictors are completely excluded by setting their coefficients to zero |\n",
    "| **When to use**         | Best suited for situations where **all predictors are potentially relevant**, and the goal is to reduce overfitting rather than eliminate features | Ideal when you suspect that only a **subset of predictors** is important, and the model should focus on those while ignoring the irrelevant ones |\n",
    "| **Output model**        | Produces a model that includes **all features**, but their coefficients are smaller in magnitude to prevent overfitting | Produces a model that is **simpler**, retaining only the most significant features and ignoring the rest by setting their coefficients to zero |\n",
    "| **Impact on Prediction**| Reduces the magnitude of coefficients, shrinking them towards zero, but does not set any coefficients exactly to zero. All predictors remain in the model | Shrinks some coefficients to **exactly zero**, effectively removing their influence. This leads to a simpler model with fewer features |\n",
    "| **Computation**         | Generally faster as it doesnâ€™t involve feature selection                      | May be slower due to the feature selection process                             |\n",
    "| **Example Use Case**    | Use when you have many predictors, all contributing to the outcome (e.g., predicting house prices where all features like size, location, etc., matter) | Use when you believe only some predictors are truly important (e.g., genetic studies where only a few genes out of thousands are relevant) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71ec972",
   "metadata": {},
   "source": [
    "## ðŸ“Œ When to Use Ridge Regression?\n",
    "Ridge Regression is most suitable when **all predictors are expected to contribute** to the outcome and none should be excluded from the model.  \n",
    "\n",
    "- Reduces **overfitting** by shrinking the coefficients, ensuring they donâ€™t become too large.  \n",
    "- Keeps **all predictors** in the model, but with controlled influence.  \n",
    "\n",
    "**Example:**  \n",
    "When predicting **house prices**, features like:  \n",
    "- Size  \n",
    "- Number of bedrooms  \n",
    "- Location  \n",
    "- Year built  \n",
    "\n",
    "â€¦are all likely relevant. Ridge Regression ensures these features remain in the model but with reduced influence to create a **balanced and robust prediction**.  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Œ When to Use Lasso Regression?\n",
    "Lasso Regression is ideal when you suspect that **only a few predictors are truly important**, and the rest may add noise or redundancy.  \n",
    "\n",
    "- Performs **automatic feature selection**.  \n",
    "- Shrinks the coefficients of less important predictors to **zero**, effectively removing them from the model.  \n",
    "\n",
    "**Example:**  \n",
    "If youâ€™re building a model with **hundreds of potential predictors** (like customer behavior features in marketing), Lasso can automatically select only the most impactful ones while ignoring the rest.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bfb33c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myEnvir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
