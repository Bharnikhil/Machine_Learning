{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9d1441f",
   "metadata": {},
   "source": [
    "# ðŸ”¹ Linear Regression\n",
    "\n",
    "**Linear Regression** is one of the simplest and most widely used algorithms in **supervised machine learning**.  \n",
    "It is mainly used for **regression tasks** â†’ predicting a **continuous value**.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ¨ Key Idea\n",
    "\n",
    "Linear regression tries to model the relationship between:\n",
    "\n",
    "- **Independent variable(s) (X):** Input(s)  \n",
    "- **Dependent variable (Y):** Output (target)  \n",
    "\n",
    "using a **straight-line equation**:\n",
    "\n",
    "\\[\n",
    "Y = mX + c\n",
    "\\]\n",
    "\n",
    "where:  \n",
    "- \\( Y \\) = predicted output  \n",
    "- \\( X \\) = input feature  \n",
    "- \\( m \\) = slope (coefficient/weight)  \n",
    "- \\( c \\) = intercept (bias)  \n",
    "\n",
    "For multiple features:\n",
    "\n",
    "\\[\n",
    "Y = w_1X_1 + w_2X_2 + \\dots + w_nX_n + b\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¸ Example: House Price Prediction\n",
    "\n",
    "Suppose we want to predict **house price** based on its **size (sqft)**.\n",
    "\n",
    "| Size (sqft) | Price (Lakhs) |\n",
    "|-------------|---------------|\n",
    "| 1000        | 50            |\n",
    "| 1500        | 65            |\n",
    "| 2000        | 80            |\n",
    "\n",
    "- The algorithm fits a **straight line** through the data points.  \n",
    "- Equation might look like:  \n",
    "\n",
    "\\[\n",
    "Price = 0.03 \\times (Size) + 20\n",
    "\\]\n",
    "\n",
    "- For a **1700 sqft** house â†’ Predicted price â‰ˆ **71 Lakhs**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27b12c4",
   "metadata": {},
   "source": [
    "# ðŸ“ˆ Linear Regression Concepts\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Best Fit Line\n",
    "- The line that minimizes error between actual points and predicted values.\n",
    "- Equation:  \n",
    "  $$\n",
    "  y = mx + c\n",
    "  $$\n",
    "- General hypothesis form:  \n",
    "  $$\n",
    "  h_\\theta(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\dots + \\theta_nx_n\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Hypothesis vs Actual\n",
    "- **Hypothesis (Prediction):** $h_\\theta(x)$ â†’ Predicted value from model  \n",
    "- **Actual Point:** $y$ â†’ True data value  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Parameters\n",
    "- **Slope ($m$ or $\\theta_1$):** Change in $y$ for one unit change in $x$  \n",
    "- **Intercept ($c$ or $\\theta_0$):** Value of $y$ when $x = 0$  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Cost Function (Error)\n",
    "Measures how far predictions are from actual values.  \n",
    "For Linear Regression, we use **Mean Squared Error (MSE):**\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} \\Big(h_\\theta(x^{(i)}) - y^{(i)}\\Big)^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Residuals\n",
    "The error for each data point:  \n",
    "\n",
    "$$\n",
    "\\text{Residual} = h_\\theta(x) - y\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Gradient Descent\n",
    "Optimization algorithm to minimize the cost function.  \n",
    "Update rule for each parameter $\\theta_j$:\n",
    "\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j}\n",
    "$$\n",
    "\n",
    "- $\\alpha$ = Learning rate (step size)  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Global Minima\n",
    "- The point where cost function $J(\\theta)$ is **minimum**.  \n",
    "- Our goal = adjust parameters until we reach this point.  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Convergence Theorem\n",
    "- Repeat gradient descent updates until cost function stops decreasing (or changes are negligible).  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Learning Rate ($\\alpha$)\n",
    "- Controls step size in gradient descent.  \n",
    "- Too small â†’ very slow convergence  \n",
    "- Too large â†’ may overshoot and never converge  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Why adjust parameters ($\\theta$)?\n",
    "- To move step by step towards the **global minima**  \n",
    "- At global minima, we get the **best-fit line** with **minimum error**  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myEnvir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
