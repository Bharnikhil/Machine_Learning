{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "036bf254",
   "metadata": {},
   "source": [
    "# ðŸ”¹ Logistic Regression Recap\n",
    "\n",
    "Logistic regression is for binary classification.  \n",
    "\n",
    "Hypothesis (prediction function):  \n",
    "$h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^T x}}$  \n",
    "\n",
    "Here $\\theta^T x = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\dots$  \n",
    "\n",
    "Output $h_\\theta(x)$ is the probability that $y=1$.  \n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ”¹ Loss Function (per example)\n",
    "\n",
    "For a single training example:  \n",
    "\n",
    "$L(y,h_\\theta(x)) = -[y\\log(h_\\theta(x)) + (1-y)\\log(1-h_\\theta(x))]$  \n",
    "\n",
    "This penalizes wrong predictions strongly (especially when the model is confident but wrong).  \n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ”¹ Cost Function (all examples)\n",
    "\n",
    "For $n$ training examples:  \n",
    "\n",
    "$J(\\theta) = -\\frac{1}{n} \\sum_{i=1}^n [y^{(i)}\\log(h_\\theta(x^{(i)})) + (1-y^{(i)})\\log(1-h_\\theta(x^{(i)}))]$  \n",
    "\n",
    "This is just the average log loss across the dataset.  \n",
    "\n",
    "$J(\\theta)$ is convex â†’ one global minimum.  \n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ”¹ Convergence\n",
    "\n",
    "We minimize $J(\\theta)$ using Gradient Descent.  \n",
    "\n",
    "Update rule:  \n",
    "$\\theta := \\theta - \\alpha \\nabla_\\theta J(\\theta)$  \n",
    "\n",
    "where $\\alpha$ = learning rate.  \n",
    "\n",
    "Since $J(\\theta)$ is convex, gradient descent converges to the global minimum (if $\\alpha$ is set properly).  \n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ”¹ Curves\n",
    "\n",
    "**Sigmoid curve (hypothesis $h_\\theta(x)$):**  \n",
    "S-shaped curve mapping $\\theta^T x$ to probabilities in (0,1).  \n",
    "\n",
    "**Loss function shape (per example):**  \n",
    "- If $y=1$: $L = -\\log(h_\\theta(x))$ â†’ high cost when $h_\\theta(x)$ is near 0, low when close to 1.  \n",
    "- If $y=0$: $L = -\\log(1-h_\\theta(x))$ â†’ high cost when $h_\\theta(x)$ is near 1, low when close to 0.  \n",
    "\n",
    "**Cost function $J(\\theta)$:**  \n",
    "Convex bowl-like shape â†’ ensures convergence to a unique minimum.  \n",
    "\n",
    "---\n",
    "\n",
    "# âœ… Summary Table (with your notation)\n",
    "\n",
    "| Concept | Formula / Shape |\n",
    "|---------|-----------------|\n",
    "| Hypothesis | $h_\\theta(x) = \\frac{1}{1+e^{-\\theta^T x}}$ |\n",
    "| Loss (single sample) | $L(y,h_\\theta(x)) = -[y\\log(h_\\theta(x)) + (1-y)\\log(1-h_\\theta(x))]$ |\n",
    "| Cost (all samples) | $J(\\theta) = -\\frac{1}{n}\\sum_{i=1}^n [y^{(i)}\\log(h_\\theta(x^{(i)})) + (1-y^{(i)})\\log(1-h_\\theta(x^{(i)}))]$ |\n",
    "| Shape | Convex â†’ global minimum |\n",
    "| Convergence | Gradient descent updates $\\theta$ until $J(\\theta)$ minimized |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e27a8d",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Why not use a simple error like linear regression?\n",
    "\n",
    "If we try to use **Mean Squared Error (MSE)** as the cost function with logistic regression, two problems occur:  \n",
    "\n",
    "1. **Non-convexity** â€“ The cost surface becomes **non-convex** (bumpy), meaning gradient descent can get stuck in local minima.  \n",
    "2. **Bad gradient behavior** â€“ Because of the sigmoid function, the derivative interacts poorly with squared error, making learning very slow or unstable.  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Why use log loss?\n",
    "\n",
    "The log-based cost function (also called **log loss / cross-entropy loss**) fixes these issues.  \n",
    "\n",
    "### âœ… Convexity  \n",
    "The function  \n",
    "\n",
    "`J(Î¸) = âˆ’(1/n) Î£ [y log(hÎ¸(x)) + (1 âˆ’ y) log(1 âˆ’ hÎ¸(x))]`  \n",
    "\n",
    "is **convex** in terms of Î¸. That guarantees **one unique global minimum** â†’ gradient descent always works.  \n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Probabilistic interpretation  \n",
    "- `hÎ¸(x)` is the probability that `y=1`.  \n",
    "- Maximizing this likelihood leads directly to the **log cost function**.  \n",
    "- So, logistic regression isnâ€™t just a classifier â€” itâ€™s a **maximum likelihood estimator**.  \n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Strong penalty for confident wrong predictions  \n",
    "- If `y=1` but `hÎ¸(x)` is close to 0 â†’ log term goes to **âˆ’âˆž** â†’ huge penalty.  \n",
    "- If `y=0` but `hÎ¸(x)` is close to 1 â†’ again huge penalty.  \n",
    "- This pushes the model to be **correct when itâ€™s confident**.  \n",
    "\n",
    "---\n",
    "\n",
    "âš¡ In short: **Log loss gives us convexity, probabilistic meaning, and strong error penalization â€” making logistic regression mathematically robust and reliable.**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myEnvir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
